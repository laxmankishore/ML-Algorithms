{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### Instance based or lazy learning - lazy algorithm"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "it uses the stored instances in the memory to find the possible **y**"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "###### Nearest Neighbour\n\nFor example if we have some observations with some target labels and we are try to find  the target value of new test  observation\nThe target value y_test will be equal to  the y(target) value of the observation which is near to the actual observation\n\n- Find similar instance\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# K - Nearest neighbour Algorithm\n\n##### Training phase \n\nWe will be saving the instances(examples)\n\n##### Prediction time:\n\nwe will get test instance --- Find the training example which is near to the test instance\n\npredict the observation output as output of test observation\n\nGeneralizing  we will be find the **K** nearest training examples\n\n###### Classification problem:\n\nWe can simply predict the target value (predict the (most frequent)majority class from the available targets)\n\n###### Regression problem:\n\nBased on the target values y1 ,y2 ,y3 etc.., we will predict the average as the estimated \noutput of the test instance\n\n### **Voronoi diagram**  refer \nonline"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Euclidean distance\n\n##### to find the closeness b/w the points \nWe will opt Euclidean distance - simple (Measuring squares of distance b/w 2 points and applying square root over them) points with smallest euclidean dist are preferred\n\nHow to find the closest points quickly at run time"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "We can give **equal weights** to the attributes\n- when the scale of attributes  are similar\n- when attributes have equal range and equal value\n\nAssuming classes are spherical in shape\n\nConsider some situations: where euclidean dist may create a problem\n\n-  attribute is more imp than other \n-  attribute contains more noise than other\n\nto over come this scenerios:\n- user larger  K values\n- `use weighted dist functions` - weighted Euclidean distance (search online)"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### small K\n- used to identify fine structure of problem space \n- or when data set is too smalle\n\n### Large K\n- to make sure that model is less sensitive to noise\n- better probability estimates for discrete classes\n- large datasets"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "types \n#### Distance weighted KNN\n\n#### Locally weighted KNN (Uses Gaussian methodology)\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### There are chances that the attribuites which have less importance or not important may impact the actual target values"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Feature Reduction :\nused To over come this problems "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Reason for  Feature reduction"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Features contain information and `More information` leads to best discriminative power **which may not be true always**\n\n **Irrelavant features** In algorithms like KNN the **irrelavant features** leads introducing to NOISE\n \n **Redundant Features** which are not contributing information which may lead to degrradation in performance of learning algo (Especially when we have limited training examples and limited computing resources)\n \n **Curse of Dimensionality** A scenerio i.e when we have too many features (higher dimensions) which some times may lead to degradation of algorithm and more computational time\n "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "we need to improve accuracy or maintains accuracy reducing the complexity ----------------------------------------------\n\n\n\n## Feature selection - 1st method of reduction\n\n\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Getting subset for features\n\n**Methods:** for **feature subset selection**\n    - optimum methods\n    - Heuristic methods\n    - Randomised methods\n    \n**Evaluation**\n - unsupervised methods : Filter methods\n - Supervised methods   : Wrapper methods\n \n ### Need to find uncorrelated features\n \n \n**Heuristic Algorithm**:\n \n - Forward selection Algorithm\n     - start with empty feature set\n     - Try each of the remaining features\n     - Estimate class./Reg. error for addding each feature\n     - **Select feature that gives more improvement**\n     - Stop when no significant improvement\n     \n     \n \n\n \n - Backward selection ALgorithm\n     - Start with complete feature set\n     - Remove each feature everytime\n     - Drop Feature with less impact\n \n \n \n "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##### Selection\n\n**Univariate** Looks at each feature independent of others\n     - Pearson correlation coefficient (**r value is b/w +1 and -1** {r~0 means no correlation})\n     - F-Score\n     - Chi square\n     - signal to Noise ratio (ratio of diff in means to diff in standard deviation {large value means Strong correlation})\n**Rank features by importance**\n\n**Ranking cut-off is determined by user**\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Multivariate Feature Selection** : considers all methods simultaneously(high value of w means Strong correlation)\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Feature Extraction : 2nd method of feature reduction\n\nMapping features from **N (higher dimension) and mapping to the M(lower dimension)** which leads to faster classification"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## PCA\n**Principal Components** \n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}